{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Recap","text":"<p>Recap is a Python library that helps you build tools for data quality, data goverenance, data profiling, data lineage, data contracts, and schema conversion.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Compatible with fsspec filesystems and SQLAlchemy databases.</li> <li>Built-in support for Parquet, CSV, TSV, and JSON files.</li> <li>Includes Pandas for data profiling.</li> <li>Uses Pydantic for metadata models.</li> <li>Convenient CLI, Python API, and REST API</li> <li>No external system dependencies.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install recap-core\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Grab schemas from filesystems:</p> <pre><code>schema(\"s3://corp-logs/2022-03-01/0.json\")\n</code></pre> <p>And databases:</p> <pre><code>schema(\"snowflake://ycbjbzl-ib10693/TEST_DB/PUBLIC/311_service_requests\")\n</code></pre> <p>In a standardized format:</p> <pre><code>{\n\"fields\": [\n{\n\"name\": \"unique_key\",\n\"type\": \"VARCHAR\",\n\"nullable\": false,\n\"comment\": \"The service request tracking number.\"\n},\n{\n\"name\": \"complaint_description\",\n\"type\": \"VARCHAR\",\n\"nullable\": true,\n\"comment\": \"Service request type\"\n}\n]\n}\n</code></pre> <p>See what schemas used to look like:</p> <pre><code>schema(\"snowflake://ycbjbzl-ib10693/TEST_DB/PUBLIC/311_service_requests\", datetime(2023, 1, 1))\n</code></pre> <p>Build metadata extractors:</p> <pre><code>@registry.metadata(\"s3://{path:path}.json\", include_df=True)\n@registry.metadata(\"bigquery://{project}/{dataset}/{table}\", include_df=True)\ndef pandas_describe(df: DataFrame, *_) -&gt; BaseModel:\n    description_dict = df.describe(include=\"all\")\n    return PandasDescription.parse_obj(description_dict)\n</code></pre> <p>Crawl your data:</p> <pre><code>crawl(\"s3://corp-logs\")\ncrawl(\"bigquery://floating-castle-728053\")\n</code></pre> <p>And read the results:</p> <pre><code>search(\"json_extract(metadata_obj, '$.count') &gt; 9999\", PandasDescription)\n</code></pre> <p>See where data comes from:</p> <pre><code>writers(\"bigquery://floating-castle-728053/austin_311/311_service_requests\")\n</code></pre> <p>And where it's going:</p> <pre><code>readers(\"bigquery://floating-castle-728053/austin_311/311_service_requests\")\n</code></pre> <p>All cached in Recap's catalog.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the Quickstart page to get started.</p>"},{"location":"cli/","title":"Recap CLI","text":"<p>Execute Recap's CLI using the <code>recap</code> command. The CLI allows you to crawl, search, and read metadata from live systems (using <code>--refresh</code>) and Recap's catalog.</p>"},{"location":"cli/#recap","title":"recap","text":"<p>Recap's command line interface.</p> <p>Usage:</p> <pre><code> [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n</code></pre>"},{"location":"cli/#crawl","title":"crawl","text":"<p>Recursively crawl a URL and its children, storing metadata and relationships in Recap's data catalog.</p> <p>Usage:</p> <pre><code> crawl [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL         [required]\n  --arg TEXT  Arbitrary options (`--arg foo=bar`) passed to the crawler.\n</code></pre>"},{"location":"cli/#ls","title":"ls","text":"<p>List a URL's child URLs.</p> <p>Usage:</p> <pre><code> ls [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL                             [required]\n  --arg TEXT                      Arbitrary options (`--arg foo=bar`) passed\n                                  to the catalog.\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see what a URL's children\n                                  used to be.\n  --refresh / --no-refresh        Skip Recap's catalog and read the latest\n                                  data directly from the URL.  [default: no-\n                                  refresh]\n</code></pre>"},{"location":"cli/#readers","title":"readers","text":"<p>See what reads from a URL.</p> <p>Usage:</p> <pre><code> readers [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL                             [required]\n  --arg TEXT                      Arbitrary options (`--arg foo=bar`) passed\n                                  to the catalog.\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see what a URL's readers used\n                                  to be.\n  --refresh / --no-refresh        Skip Recap's catalog and read the latest\n                                  data directly from the URL.  [default: no-\n                                  refresh]\n</code></pre>"},{"location":"cli/#reads","title":"reads","text":"<p>See what a URL reads. URLs must be accounts or jobs.</p> <p>Usage:</p> <pre><code> reads [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL                             [required]\n  --arg TEXT                      Arbitrary options (`--arg foo=bar`) passed\n                                  to the catalog.\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see what a URL used to read.\n  --refresh / --no-refresh        Skip Recap's catalog and read the latest\n                                  data directly from the URL.  [default: no-\n                                  refresh]\n</code></pre>"},{"location":"cli/#schema","title":"schema","text":"<p>Get a Recap schema for a URL.</p> <p>Usage:</p> <pre><code> schema [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL                             [required]\n  --arg TEXT                      Arbitrary options (`--arg foo=bar`) passed\n                                  to the catalog.\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see what a URL's schema used\n                                  to be.\n  --refresh / --no-refresh        Skip Recap's catalog and read the latest\n                                  data directly from the URL.  [default: no-\n                                  refresh]\n</code></pre>"},{"location":"cli/#search","title":"search","text":"<p>Usage:</p> <pre><code> search [OPTIONS] METADATA_TYPE QUERY\n</code></pre> <p>Options:</p> <pre><code>  METADATA_TYPE                   [required]\n  QUERY                           [required]\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see what a URL's metadata\n                                  used to be.\n</code></pre>"},{"location":"cli/#serve","title":"serve","text":"<p>Starts Recap's HTTP/JSON API server.</p> <p>Usage:</p> <pre><code> serve [OPTIONS]\n</code></pre>"},{"location":"cli/#writers","title":"writers","text":"<p>See what writes to a URL.</p> <p>Usage:</p> <pre><code> writers [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL                             [required]\n  --arg TEXT                      Arbitrary options (`--arg foo=bar`) passed\n                                  to the catalog.\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see what a URL's writers used\n                                  to be.\n  --refresh / --no-refresh        Skip Recap's catalog and read the latest\n                                  data directly from the URL.  [default: no-\n                                  refresh]\n</code></pre>"},{"location":"cli/#writes","title":"writes","text":"<p>See what a URL writes. URLs must be accounts or jobs.</p> <p>Usage:</p> <pre><code> writes [OPTIONS] URL\n</code></pre> <p>Options:</p> <pre><code>  URL                             [required]\n  --arg TEXT                      Arbitrary options (`--arg foo=bar`) passed\n                                  to the catalog.\n  --time [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]\n                                  Time travel to see where a URL's used to\n                                  write.\n  --refresh / --no-refresh        Skip Recap's catalog and read the latest\n                                  data directly from the URL.  [default: no-\n                                  refresh]\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#install","title":"Install","text":"<p>Start by installing Recap. Python 3.10 is required.</p> <pre><code>pip install recap-core\n</code></pre>"},{"location":"quickstart/#crawl","title":"Crawl","text":"<p>Now let's crawl a database:</p> CLIPython <pre><code>recap crawl postgresql://username@localhost/some_db\n</code></pre> <pre><code>from recap.repl import *\n\ncrawl(\"postgresql://username@localhost/some_db\")\n</code></pre> <p>You can use any SQLAlchemy connect string.</p> CLIPython <pre><code>recap crawl bigquery://some-project-12345\nrecap crawl snowflake://username:password@account_identifier/SOME_DB/SOME_SCHEMA?warehouse=SOME_COMPUTE\n</code></pre> <pre><code>from recap.repl import *\n\ncrawl(\"bigquery://some-project-12345\")\ncrawl(\"snowflake://username:password@account_identifier/SOME_DB/SOME_SCHEMA?warehouse=SOME_COMPUTE\")\n</code></pre> <p>Note</p> <p>You must install appropriate drivers and SQLAlchemy dialects for the databases you wish to crawl. For PostgreSQL, you'll have to <code>pip install psycopg2</code>. For Snowflake and BigQuery, you'll have to <code>pip install snowflake-sqlalchemy</code> and <code>pip install sqlalchemy-bigquery</code>, respectively.</p> <p>You can also crawl filesystems and object stores.</p> CLIPython <pre><code>recap crawl /tmp/data\nrecap crawl s3://power-analysis-ready-datastore\n</code></pre> <pre><code>from recap.repl import *\n\ncrawl(\"/tmp/data\")\ncrawl(\"s3://power-analysis-ready-datastore\")\n</code></pre> <p>Note</p> <p>You must install appropriate fsspec implementation for the filesystem you wish to crawl.</p>"},{"location":"quickstart/#list","title":"List","text":"<p>Crawled metadata is stored in a directory structure. See what's available using:</p> CLIPython <pre><code>recap ls /tmp/data\n</code></pre> <pre><code>from recap.repl import *\n\nls(\"/tmp/data\")\n</code></pre> <p>Recap will respond with a JSON list in the CLI:</p> <pre><code>[\n\"file:///tmp/data/foo\",\n\"file:///tmp/data/bar.json\"\n]\n</code></pre>"},{"location":"quickstart/#read","title":"Read","text":"<p>After you poke around, try and read some metadata.</p> CLIPython <pre><code>recap schema file:///tmp/data/foo.json\n</code></pre> <pre><code>from recap.repl import *\n\nschema(\"/tmp/data/foo.json\")\n</code></pre> <p>Recap will print <code>foo.json</code>'s inferred schema to the CLI in JSON format:</p> <pre><code>{\n\"fields\": [\n{\n\"name\": \"test\",\n\"type\": \"string\",\n\"default\": null,\n\"nullable\": null,\n\"comment\": null\n}\n]\n}\n</code></pre>"},{"location":"quickstart/#time","title":"Time","text":"<p>Recap keeps historical data. You can set the <code>time</code> parameter to see what data looked like at specific point in time. This is useful for debugging data quality issues.</p> CLIPython <pre><code>recap schema file:///tmp/data/foo.json --time 2020-02-22\n</code></pre> <pre><code>from recap.repl import *\n\nschema(\"/tmp/data/foo.json\", datetime(2022, 2, 22))\n</code></pre>"},{"location":"quickstart/#search","title":"Search","text":"<p>Recap stores its metadata in SQLite by default. You can use SQLite's json_extract syntax to search the catalog:</p> CLIPython <pre><code>recap search schema \"json_extract(metadata_obj, '$.fields') IS NOT NULL\"\n</code></pre> <pre><code>from recap.repl import *\n\nsearch(\"json_extract(metadata_obj, '$.fields') IS NOT NULL\")\n</code></pre> <p>The database file defaults to <code>~/.recap/recap.db</code>, if you wish to open a SQLite client directly.</p>"},{"location":"quickstart/#integrations","title":"Integrations","text":"<p>See the Integrations page to see all of the systems Recap supports, and what data you can crawl.</p>"},{"location":"rest/","title":"REST API","text":"<p>Recap comes with an HTTP/JSON API server implementation of Recap's storage interface. The server allows non-Python systems to integrate with Recap, and different systems to share their metadata when they all point to the same server.</p>"},{"location":"rest/#starting","title":"Starting","text":"<p>Execute <code>recap serve</code> to start Recap's server.</p>"},{"location":"rest/#configuring","title":"Configuring","text":"<p>Recap's server is implemented as a FastAPI in a uvicorn web server. You can configure uvicorn using the <code>RECAP_UVICORN_SETTINGS</code> environment variable.</p> <pre><code>RECAP_UVICORN_SETTINGS='{\"port\": 9000}'\n</code></pre>"},{"location":"rest/#endpoints","title":"Endpoints","text":"<p>Recap's JSON schema is visible at http://localhost:8000/openapi.json. API documentation is also visible at http://localhost:8000/docs and http://localhost:8000/redoc.</p>"},{"location":"rest/#examples","title":"Examples","text":"<p>The following examples illlustrate how to call a Recap server running at http://localhost:8000.</p>"},{"location":"rest/#write-a-schema","title":"Write a Schema","text":"<pre><code>curl -X PUT 'http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/some_table/metadata/schema' \\\n-d '{\"fields\": [{\"name\": \"test\"}]}' \\\n-H \"Content-Type: application/json\"\n</code></pre>"},{"location":"rest/#read-a-schema","title":"Read a Schema","text":"<pre><code>curl 'http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/some_table/metadata/schema'\n</code></pre>"},{"location":"rest/#write-a-relationship","title":"Write a Relationship","text":"<pre><code>curl -X POST 'http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/links/contains?relationship=contains&amp;other_url=http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/some_table'\n</code></pre>"},{"location":"rest/#read-a-relationship","title":"Read a Relationship","text":"<pre><code>curl 'http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/links/contains'\n</code></pre>"},{"location":"rest/#delete-a-relationship","title":"Delete a Relationship","text":"<pre><code>curl -X DELETE 'http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/links/contains?relationship=contains&amp;other_url=http://localhost:8000/storage/postgresql://localhost/some_db/some_schema/some_table'\n</code></pre>"},{"location":"api/recap.catalog/","title":"recap.catalog","text":""},{"location":"api/recap.catalog/#recap.catalog.Catalog","title":"<code>Catalog</code>","text":"<p>Recap's catalog defines standard metadata and relationships between URLs. The only metadata the catalog models is <code>recap.metadata.Schema</code>.</p> <p>Relationships are modeled as:</p> <ul> <li><code>account</code> runs <code>job</code></li> <li><code>account</code> reads <code>data</code></li> <li><code>account</code> writes <code>data</code></li> <li><code>job</code> reads <code>data</code></li> <li><code>job</code> writes <code>data</code></li> <li><code>data</code> contains <code>data</code></li> </ul> <p><code>data</code> contains <code>data</code> might seem odd at first, but it simply models how object stores like S3 behave. Databases and tables are all treated as <code>data</code>.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.ls","title":"<code>ls(url, time=None, refresh=False, **kwargs)</code>","text":"<p>Lists all URLs that the provided URL contains. In a filesystem, <code>ls</code> would return a path's children.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Fetch the contained children for this URL.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return children from the last crawl before the provided timestamp.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Bypass the catalog's cache, fetching the data directly from the URL. Cache the results in the catalog.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of URLs contained by the input URL. None if the input URL doesn't exist.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.readers","title":"<code>readers(url, time=None, refresh=False, **kwargs)</code>","text":"<p>Lists all URLs that read from the input URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Fetch readers for this this URL.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return readers from the last crawl before the provided timestamp.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Bypass the catalog's cache, fetching the data directly from the URL. Cache the results in the catalog.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of URLs that read from the input URL. If the return URL represents an account, then the account has read-access to the input URL. If the return URL represents a job, then the account reads the input URL's data.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.reads","title":"<code>reads(url, time=None, refresh=False, **kwargs)</code>","text":"<p>Lists all URLs that the input URL reads.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Fetch URLs that this URL reads.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return URLs from the last crawl before the provided timestamp.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Bypass the catalog's cache, fetching the data directly from the URL. Cache the results in the catalog.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of URLs that the input URL has read from. If the input URL is an account, the account will have read-access to the returned URLs. If the input URL is a job, the job has read from the return URLs.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.schema","title":"<code>schema(url, time=None, refresh=False, **kwargs)</code>","text":"<p>Returns a schema for the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch schema for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return a scheam from the last crawl before the provided timestamp.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Bypass the catalog's cache, fetching the schema directly from the URL. Cache the results in the catalog.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema | None</code> <p>A schema for the URL or <code>None</code> if the URL doesn't exist.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.search","title":"<code>search(query, metadata_type, time=None)</code>","text":"<p>Search for metadata in the catallog.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to apply to the storage layer. This is usually a <code>WHERE</code> clause, for example: <code>\"url='file:///tmp/recap-test/foo/foo.json'\"</code> or <code>\"json_extract(metadata_obj, '$.fields.name') = 'email'\"</code></p> required <code>metadata_type</code> <code>type[MetadataSubtype]</code> <p>The type of metadata to search.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return URLs from the last crawl before the provided timestamp.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[MetadataSubtype]</code> <p>A list of Pydantic <code>BaseModel</code> metadata objects that match the search query.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.writers","title":"<code>writers(url, time=None, refresh=False, **kwargs)</code>","text":"<p>List all URLs that write to the input URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL that is written to.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return URLs from the last crawl before the provided timestamp.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Bypass the catalog's cache, fetching the data directly from the URL. Cache the results in the catalog.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of URLs that have written to the input URL. If the return URL is an account, the account will have write-access to the input URL. If the return URL is a job, the job has written to the input URL.</p>"},{"location":"api/recap.catalog/#recap.catalog.Catalog.writes","title":"<code>writes(url, time=None, refresh=False, **kwargs)</code>","text":"<p>List all URLs that the input URL writes to.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Fetch output URLs written to by this input URL.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to a point in time. Causes the method to return URLs from the last crawl before the provided timestamp.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Bypass the catalog's cache, fetching the data directly from the URL. Cache the results in the catalog.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of URLs that the input URL writes to. If the input URL is an account, the account will have write-access to the return URLs. If the input URL is a job, the job has written to the return URLs.</p>"},{"location":"api/recap.catalog/#recap.catalog.Relationship","title":"<code>Relationship</code>","text":"<p>         Bases: <code>Enum</code></p> <p>A relationship between two URLs in the catalog.</p>"},{"location":"api/recap.catalog/#recap.catalog.Relationship.CONTAINS","title":"<code>CONTAINS = 1</code>  <code>class-attribute</code>","text":"<p>The source URL contains the destination URL. This is used to model data hierarchy. Folders contain files, databases contain schemas, and so on.</p>"},{"location":"api/recap.catalog/#recap.catalog.Relationship.READS","title":"<code>READS = 3</code>  <code>class-attribute</code>","text":"<p>The source URL reads the destination URL. READS is used to model both account access and job lineage. When the source URL is an account, READS signals that the account has read-access to the destination URL. When the source URL is a job, READS signals that the job reads the destination URL's data.</p>"},{"location":"api/recap.catalog/#recap.catalog.Relationship.RUNS","title":"<code>RUNS = 2</code>  <code>class-attribute</code>","text":"<p>The source URL runs the destination URL. RUNS is usually used to mode job executation: an account runs a job.</p>"},{"location":"api/recap.catalog/#recap.catalog.Relationship.WRITES","title":"<code>WRITES = 4</code>  <code>class-attribute</code>","text":"<p>The source URL writes to the destination URL. WRITES is used to model both account access and job lineage. When the source URL is an account, WRITES signals that the account has write-access to the destination URL. When the source URL is a job, WRITES signals that the job writes to the destination URL's data.</p>"},{"location":"api/recap.catalog/#recap.catalog.create_catalog","title":"<code>create_catalog(url=None, registry=None, **storage_opts)</code>","text":"<p>A helper that creates a catalog and its underlying storage layer.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | None</code> <p>URL for the storage layer. If unset, the storage layer uses its default (A SQLite DB in <code>$RECAP_HOME/recap.db</code>).</p> <code>None</code> <code>registry</code> <code>FunctionRegistry | None</code> <p>A function registry containting metadata and relationship functions. If unset, the global registry is used.</p> <code>None</code> <code>storage_opts</code> <p>Extra options passed through to the storage layer.</p> <code>{}</code>"},{"location":"api/recap.catalog/#recap.catalog.safe","title":"<code>safe(url)</code>","text":"<p>Strips the <code>username:password</code> from a URL.</p> <p>Given:</p> <pre><code>scheme://user:pass@host:port/some/path;param?some=query#fragment\n</code></pre> <p>Returns:</p> <pre><code>scheme://host:port/some/path;param?some=query#fragment\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to make safe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A URL with the <code>username:password</code> removed.</p>"},{"location":"api/recap.integrations/","title":"recap.integrations","text":""},{"location":"api/recap.integrations/#recap.integrations.bigquery","title":"<code>bigquery</code>","text":""},{"location":"api/recap.integrations/#recap.integrations.bigquery.ls","title":"<code>ls(engine, project, dataset=None, **_)</code>","text":"<p>List all URLs contained by a project and (optionally) a dataset. If project is set, returned URLs will be datasets:     <code>bigquery://some-project-1234/some_dataset</code> If dataset is set, returned URLs will be tables:     <code>bigquery://some-project-1234/some_dataset/some_table</code></p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy Engine to use when inspecting schemas and tables.</p> required <code>project</code> <code>str</code> <p>A google cloud project ID (e.g. <code>some-project-1234</code>)</p> required <code>dataset</code> <code>str | None</code> <p>A dataset name.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset or table URIs.</p>"},{"location":"api/recap.integrations/#recap.integrations.bigquery.readers","title":"<code>readers(engine, project, dataset, table, **client_args)</code>","text":"<p>Returns all accounts and jobs that read from a BigQuery table. Return URLs will be of the form:</p> <pre><code>bigquery://some-project?account=user:some@email.com\nbigquery://some-project?job=bquxjob123456\n</code></pre> <p>A reader account signals read-access. A reader job signals that the job read from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy Engine to use when inspecting schemas and tables.</p> required <code>project</code> <code>str</code> <p>A google cloud project ID (e.g. <code>some-project-1234</code>)</p> required <code>dataset</code> <code>str</code> <p>A dataset name.</p> required <code>table</code> <code>str</code> <p>A table name.</p> required <code>args</code> <p>Extra arguments passed to a <code>google.cloud.bigquery.Client</code>.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset or table URIs.</p>"},{"location":"api/recap.integrations/#recap.integrations.bigquery.schema","title":"<code>schema(engine, dataset, table, **_)</code>","text":"<p>Fetch a schema from a BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy Engine to use when inspecting schemas and tables.</p> required <code>project</code> <p>A google cloud project ID (e.g. <code>some-project-1234</code>)</p> required <code>dataset</code> <code>str</code> <p>A dataset name.</p> required <code>table</code> <code>str</code> <p>A table name.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>A Recap schema.</p>"},{"location":"api/recap.integrations/#recap.integrations.bigquery.writers","title":"<code>writers(engine, project, dataset, table, **client_args)</code>","text":"<p>Returns all accounts and jobs that write to a BigQuery table. Return URLs will be of the form:</p> <pre><code>bigquery://some-project?account=user:some@email.com\nbigquery://some-project?job=bquxjob123456\n</code></pre> <p>A writer account signals write-access. A writer job signals that the job wrote to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy Engine to use when inspecting schemas and tables.</p> required <code>project</code> <code>str</code> <p>A google cloud project ID (e.g. <code>some-project-1234</code>)</p> required <code>dataset</code> <code>str</code> <p>A dataset name.</p> required <code>table</code> <code>str</code> <p>A table name.</p> required <code>client_args</code> <p>Extra arguments passed to a <code>google.cloud.bigquery.Client</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset or table URIs.</p>"},{"location":"api/recap.integrations/#recap.integrations.fsspec","title":"<code>fsspec</code>","text":""},{"location":"api/recap.integrations/#recap.integrations.fsspec.ls","title":"<code>ls(url, fs, path=None)</code>","text":"<p>List all children in a filesystem path. Recap treats all filesystem paths as objects (similar to S3), so each URL might contain data and/or child URLs.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The fully matched URL when using the function registry.</p> required <code>fs</code> <code>AbstractFileSystem</code> <p>A <code>fsspec</code> filesystem.</p> required <code>path</code> <code>str | None</code> <p>Filesystem path.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of child URLs.</p>"},{"location":"api/recap.integrations/#recap.integrations.fsspec.schema","title":"<code>schema(url, path, **_)</code>","text":"<p>Fetch a Recap schema for a URL. This method supports S3 and local filesystems, and CSV, TSV, Parquet, and JSON filetypes.</p> <p>Recap uses <code>frictionless</code> for schema inferrence.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The fully matched URL when using the function registry.</p> required <code>path</code> <code>str</code> <p>Path to a CSV, TSV, Parquet, or JSON file.</p> required"},{"location":"api/recap.integrations/#recap.integrations.sqlalchemy","title":"<code>sqlalchemy</code>","text":"<p>This module uses INFORMATION_SCHEMA's terminology and hierarchy, which models database hierarchy as:</p> <pre><code>catalog -&gt; schema -&gt; table -&gt; column\n</code></pre> <p>If you're using PostgreSQL, \"schema\" is usually \"public\". If you're using MySQL, \"catalog\" is always hard-coded to \"def\", and usually excluded from MySQL connect URLs. In BigQuery, \"catalog\" is the project and \"schema\"is the dataset.</p>"},{"location":"api/recap.integrations/#recap.integrations.sqlalchemy.ls","title":"<code>ls(engine, schema=None, **_)</code>","text":"<p>Fetch (INFORMATION_SCHEMA) schemas or tables from a database.</p> <p>URLs are of the form:</p> <pre><code>postgresql://{netloc}/{database}/{schema}/{table}\nsnowflake://{netloc}/{database}/{schema}/{table}\nmysql://{netloc}/{schema}/{table}\nbigquery://{project}/{dataset}/{table}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy Engine to use when inspecting schemas and tables.</p> required <code>schema</code> <code>str | None</code> <p>A database schema.</p> <code>None</code> <code>table</code> <p>A table name.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A Recap schema.</p>"},{"location":"api/recap.integrations/#recap.integrations.sqlalchemy.schema","title":"<code>schema(engine, schema, table, **_)</code>","text":"<p>Fetch a Recap schema for a SQL table.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy Engine to use when inspecting schemas and tables.</p> required <code>schema</code> <code>str</code> <p>A database schema.</p> required <code>table</code> <code>str</code> <p>A table name.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>A Recap schema.</p>"},{"location":"api/recap.metadata/","title":"recap.metadata","text":"<p>This module contains the core metadata models that Recap understands. All models extend Pydantic's <code>BaseModel</code> class.</p> <p>Right now, Recap's only metadata model is a Schema. Other entities, such as accounts and jobs, are represented by URLs, but have no associated metadata.</p>"},{"location":"api/recap.metadata/#recap.metadata.Field","title":"<code>Field</code>","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/recap.metadata/#recap.metadata.Field.comment","title":"<code>comment: str | None = None</code>  <code>class-attribute</code>","text":"<p>A documentation comment for the field.</p>"},{"location":"api/recap.metadata/#recap.metadata.Field.default","title":"<code>default: str | None = None</code>  <code>class-attribute</code>","text":"<p>A field's default value (represented as a string).</p>"},{"location":"api/recap.metadata/#recap.metadata.Field.name","title":"<code>name: str</code>  <code>class-attribute</code>","text":"<p>The name of a field.</p>"},{"location":"api/recap.metadata/#recap.metadata.Field.nullable","title":"<code>nullable: bool | None = None</code>  <code>class-attribute</code>","text":"<p>Whether the field is nullable or not. If <code>False</code>, the field is required.</p>"},{"location":"api/recap.metadata/#recap.metadata.Field.type","title":"<code>type: str | None = None</code>  <code>class-attribute</code>","text":"<p>A field's type.</p>"},{"location":"api/recap.metadata/#recap.metadata.Schema","title":"<code>Schema</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Recap's representation of a Schema.</p>"},{"location":"api/recap.metadata/#recap.metadata.Schema.fields","title":"<code>fields: list[Field]</code>  <code>class-attribute</code>","text":"<p>Fields in the schema.</p>"},{"location":"api/recap.repl/","title":"recap.repl","text":"<p>Recap's REPL functions are a convenient way to interact with Recap in Python. The functions behave similarly to Recap's CLI, but return Python objects.</p>"},{"location":"api/recap.repl/#recap.repl.crawl","title":"<code>crawl(url, **kwargs)</code>","text":"<p>Recursively crawl a URL and its children, storing metadata and relationships in Recap's data catalog.</p>"},{"location":"api/recap.repl/#recap.repl.ls","title":"<code>ls(url, time=None, refresh=False, **kwargs)</code>","text":"<p>List a URL's child URLs.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to list children for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see what a URL's children used to be.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Skip Recap's catalog and read the latest data directly from the URL.</p> <code>False</code> <code>kwargs</code> <p>Arbitrary options passed to the crawler.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of the input URL's children.</p>"},{"location":"api/recap.repl/#recap.repl.readers","title":"<code>readers(url, time=None, refresh=False, **kwargs)</code>","text":"<p>See what reads from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch readers for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see what a URL's readers used to be.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Skip Recap's catalog and read the latest data directly from the URL.</p> <code>False</code> <code>kwargs</code> <p>Arbitrary options passed to the catalog.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list URLs that read the input URL.</p>"},{"location":"api/recap.repl/#recap.repl.reads","title":"<code>reads(url, time=None, refresh=False, **kwargs)</code>","text":"<p>See what a URL reads. URLs must be accounts or jobs.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch readers for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see what a URL used to read.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Skip Recap's catalog and read the latest data directly from the URL.</p> <code>False</code> <code>kwargs</code> <p>Arbitrary options passed to the catalog.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list URLs that the input URL reads.</p>"},{"location":"api/recap.repl/#recap.repl.schema","title":"<code>schema(url, time=None, refresh=False, **kwargs)</code>","text":"<p>Get a Recap schema for a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch a schema for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see what a URL's schema used to be.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Skip Recap's catalog and read the latest data directly from the URL.</p> <code>False</code> <code>kwargs</code> <p>Arbitrary options passed to the catalog.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Schema | None</code> <p>A list URLs that the input URL reads.</p>"},{"location":"api/recap.repl/#recap.repl.search","title":"<code>search(query, metadata_type, time=None)</code>","text":"<p>Get a Recap schema for a URL.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to apply to the storage layer. This is usually a <code>WHERE</code> clause, for example: <code>\"url='file:///tmp/recap-test/foo/foo.json'\"</code> or <code>\"json_extract(metadata_obj, '$.fields.name') = 'email'\"</code></p> required <code>metadata_type</code> <code>type[MetadataSubtype]</code> <p>The type of metadata to search for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see what a URL's metadata used to be.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[MetadataSubtype]</code> <p>A list metadata documents that match the search.</p>"},{"location":"api/recap.repl/#recap.repl.writers","title":"<code>writers(url, time=None, refresh=False, **kwargs)</code>","text":"<p>See what writes to a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch writers for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see what a URL's writers used to be.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Skip Recap's catalog and read the latest data directly from the URL.</p> <code>False</code> <code>kwargs</code> <p>Arbitrary options passed to the catalog.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list URLs that write to the input URL.</p>"},{"location":"api/recap.repl/#recap.repl.writes","title":"<code>writes(url, time=None, refresh=False, **kwargs)</code>","text":"<p>See what a URL writes. URLs must be accounts or jobs.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch writers for.</p> required <code>time</code> <code>datetime | None</code> <p>Time travel to see where a URL's used to write.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>Skip Recap's catalog and read the latest data directly from the URL.</p> <code>False</code> <code>kwargs</code> <p>Arbitrary options passed to the catalog.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list URLs that the input URL writes to.</p>"},{"location":"api/recap.storage/","title":"recap.storage","text":""},{"location":"api/recap.storage/#recap.storage.abstract","title":"<code>abstract</code>","text":""},{"location":"api/recap.storage/#recap.storage.abstract.MetadataSubtype","title":"<code>MetadataSubtype = TypeVar('MetadataSubtype', bound=BaseModel)</code>  <code>module-attribute</code>","text":"<p>A templated type that extends Pydantic's <code>BaseModel</code> class.</p>"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage","title":"<code>AbstractStorage</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract representation of Recap's storage layer.</p> <p>Recap's storage layer provides graph, search, and time travel capabilities. Nodes are represented by URLs. A URL can be linked to another with a relationship type. Metadata can be attached to a node.</p>"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage.link","title":"<code>link(url, relationship, other_url)</code>  <code>abstractmethod</code>","text":"<p>Connect two URLs in the graph with relationship type.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> required <code>relationship</code> <code>str</code> required <code>other_url</code> <code>str</code> required"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage.links","title":"<code>links(url, relationship, time=None, direction=Direction.FROM)</code>  <code>abstractmethod</code>","text":"<p>Read graph edges (links) for a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> required <code>relationship</code> <code>str</code> required <code>time</code> <code>datetime | None</code> <code>None</code> <code>direction</code> <code>Direction</code> <code>Direction.FROM</code> <p>Returns:</p> Type Description <code>list[str]</code>"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage.metadata","title":"<code>metadata(url, metadata_type, time=None)</code>  <code>abstractmethod</code>","text":"<p>Read a type of metadata for a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> required <code>metadata_type</code> <code>type[MetadataSubtype]</code> required <code>time</code> <code>datetime | None</code> <code>None</code> <p>Returns:</p> Type Description <code>MetadataSubtype | None</code>"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage.search","title":"<code>search(query, metadata_type, time=None)</code>  <code>abstractmethod</code>","text":"<p>Search for metadata.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> required <code>metadata_type</code> <code>type[MetadataSubtype]</code> required <code>time</code> <code>datetime | None</code> <code>None</code> <p>Returns:</p> Type Description <code>list[MetadataSubtype]</code>"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage.unlink","title":"<code>unlink(url, relationship, other_url)</code>  <code>abstractmethod</code>","text":"<p>Disconnect two URLs in the graph with relationship type.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> required <code>relationship</code> <code>str</code> required <code>other_url</code> <code>str</code> required <p>Returns:</p> Type Description"},{"location":"api/recap.storage/#recap.storage.abstract.AbstractStorage.write","title":"<code>write(url, metadata)</code>  <code>abstractmethod</code>","text":"<p>(Over)write a type of metadata for a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> required <code>metadata</code> <code>BaseModel</code> required"},{"location":"api/recap.storage/#recap.storage.abstract.Direction","title":"<code>Direction</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Edge direction between two nodes.</p>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>Though Recap's CLI can run without any configuration, you might want to configure Recap. Recap uses Pydantic's BaseSettings class for its configuration system.</p>"},{"location":"guides/configuration/#configs","title":"Configs","text":"<p>See Recap's config.py for all available configuration parameters.</p> <p>Commonly set environment variables include:</p> <pre><code>RECAP_STORAGE_SETTINGS__URL=http://localhost:8000/storage\nRECAP_LOGGING_CONFIG_FILE=/tmp/logging.toml\n</code></pre> <p>Note</p> <p>Note the double-underscore (dunder) in the <code>URL</code> environment variable. This is a common way to set nested dictionary and object values in Pydantic's <code>BaseSettings</code> classes. You can also set JSON objects like <code>RECAP_STORAGE_SETTINGS='{\"url\": \"http://localhost:8000/storage\"}'</code>. See Pydantic's settings management page for more information.</p>"},{"location":"guides/configuration/#dotenv","title":"Dotenv","text":"<p>Recap supports .env files to manage environment variables. Simply create a <code>.env</code> in your current working directory and use Recap as usual. Pydantic handles the rest.</p>"},{"location":"guides/configuration/#home","title":"Home","text":"<p>RECAP_HOME defines where Recap looks for storage and secret files. By default, RECAP_HOME is set to <code>~/.recap</code>.</p>"},{"location":"guides/configuration/#secrets","title":"Secrets","text":"<p>You can set environment variables with secrets in them using Pydantic's secret handling mechanism. By default, Recap looks for secrets in <code>$RECAP_HOME/.secrets</code>.</p>"},{"location":"guides/logging/","title":"Logging","text":"<p>Recap uses Python's standard logging library. Logs are printed to standard out using Rich's logging handler by default.</p>"},{"location":"guides/logging/#customizing","title":"Customizing","text":"<p>You can customize Recap's log output. Set the <code>RECAP_LOGGING_CONFIG_FILE</code> environment variable to point to a TOML file that conforms to Python's dictConfig schema.</p> <pre><code>version = 1\ndisable_existing_loggers = true\nformatters.standard.format = \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n\n[handlers.default]\nformatter = \"standard\"\nclass = \"rich.logging.RichHandler\"\nshow_time = false\nshow_level = false\nshow_path = false\n\n[loggers.\"\"]\nhandlers = ['default']\nlevel = \"WARNING\"\npropagate = false\n\n[loggers.recap]\nhandlers = ['default']\nlevel = \"INFO\"\npropagate = false\n\n[loggers.uvicorn]\nhandlers = ['default']\nlevel = \"INFO\"\npropagate = false\n</code></pre>"}]}